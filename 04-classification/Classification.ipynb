{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "This notebook demonstrates some of the classification tasks that can be accomplished using data retrieved from the HTRC API.\n",
    "\n",
    "We will be using the `scikit-learn` library to tackle these classification problems.\n",
    "\n",
    "For these examples, we will be gathering data using the advance search from the Hathi Trust library. To create your training set, determine a search query that will become your labels. Once you search, add the search results to a collection named after your label.\n",
    "\n",
    "Once you have completed adding to your collection, go to \"My Collections\" under ____. There you will find a button called \"Download Metadata\".\n",
    "\n",
    "Download the JSON file associated with your collection and place it in the local directory that you are working in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON files created\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "jsonFiles = [file for file in os.listdir('.') if file.find('json') != -1]\n",
    "\n",
    "txts = []\n",
    "for file in jsonFiles:\n",
    "    with open(file) as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "    texts = data['gathers']\n",
    "    ids = [text['htitem_id'] for text in texts]\n",
    "    \n",
    "    filename = data['title'] + '.txt'\n",
    "    txts.append(filename)\n",
    "    \n",
    "    # write each id into txt file\n",
    "    with open(filename, 'w') as f:\n",
    "        for textid in ids:\n",
    "            f.write(textid + '\\n')\n",
    "\n",
    "print(\"JSON files created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this step is complete, you can follow the instructions in the Setup notebook to load the data as needed.\n",
    "\n",
    "## Task 1: Genre Classification\n",
    "\n",
    "In this example, we'll be classifying texts into 3 different genres: Poetry, History, and Science Fiction. JSON files containing the metadata for 500 texts in each genre have been included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path extraction complete\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "history_output = !htid2rsync --f history.txt | rsync -azv --files-from=- data.sharc.hathitrust.org::features/ history/\n",
    "poetry_output = !htid2rsync --f poetry.txt | rsync -azv --files-from=- data.sharc.hathitrust.org::features/ poetry/\n",
    "scifi_output = !htid2rsync --f scifi.txt | rsync -azv --files-from=- data.sharc.hathitrust.org::features/ scifi/\n",
    "outputs = list([history_output, poetry_output, scifi_output])\n",
    "subjects = ['history', 'poetry', 'scifi']\n",
    "\n",
    "paths = {}\n",
    "suffix = '.json.bz2'\n",
    "for subject, output in zip(subjects, outputs):\n",
    "    folder = subject\n",
    "    filePaths = [path for path in output if path.endswith(suffix)]\n",
    "    paths[subject] = [os.path.join(folder, path) for path in filePaths]\n",
    "#     fn = subject + '_paths.txt'\n",
    "#     with open(fn, 'w') as f:\n",
    "#         for path in paths[subject]:\n",
    "#             p = str(path) + '\\n'\n",
    "#             f.write(p)\n",
    "\n",
    "print(\"Path extraction complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous notebooks, we'll construct `FeatureReader` objects for each corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished reading paths\n"
     ]
    }
   ],
   "source": [
    "from htrc_features import FeatureReader\n",
    "\n",
    "# paths = {}\n",
    "# subjects = ['history', 'poetry', 'scifi']\n",
    "# for subject in subjects:\n",
    "#     with open(subject + '_paths.txt', 'r') as f:\n",
    "#         paths[subject] = [line[:len(line)-1] for line in f.readlines()]\n",
    "        \n",
    "history = FeatureReader(paths['history'])\n",
    "\n",
    "poetry = FeatureReader(paths['poetry'])\n",
    "\n",
    "scifi = FeatureReader(paths['scifi'])\n",
    "\n",
    "print(\"Finished reading paths\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create our bag of words matrix, we need to keep a global dictionary of all words seen in each of our texts. We initialize \"wordDict\", which tracks all the words seen and records its index in the bag of words matrix. We also keep a list of volumes so that we can parse them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating global dictionary\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "wordDict = {}\n",
    "i = 0 \n",
    "volumes = []\n",
    "\n",
    "print(\"Generating global dictionary\")\n",
    "volCount = 0\n",
    "for vol in scifi.volumes():\n",
    "    volumes.append(vol)\n",
    "    tok_list = vol.tokenlist(pages=False)\n",
    "    tokens = tok_list.index.get_level_values('token')\n",
    "    if volCount == 200:  # first 200 from scifi volumes\n",
    "        break\n",
    "    volCount += 1 \n",
    "    \n",
    "    for token in tokens:\n",
    "        if token not in wordDict.keys():\n",
    "            wordDict[token] = i\n",
    "            i += 1\n",
    "\n",
    "for vol in poetry.volumes():\n",
    "    volumes.append(vol)\n",
    "    tok_list = vol.tokenlist(pages=False)\n",
    "    tokens = tok_list.index.get_level_values('token')\n",
    "    if volCount == 400:  # additional 200 from poetry volumes\n",
    "        break\n",
    "    volCount += 1 \n",
    "    \n",
    "    for token in tokens:\n",
    "        if token not in wordDict.keys():\n",
    "            wordDict[token] = i\n",
    "            i += 1\n",
    "\n",
    "print(\"Global dictionary generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "How would you change the above code to have 500 training volumes per class?\n",
    "\n",
    "---\n",
    "\n",
    "Once we construct the global dictionary, we can fill the bag of words matrix with the word counts for each volume. Once we have this, we will use it to format the training data for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating bag of words matrix\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating bag of words matrix\")\n",
    "dtm = np.zeros((volCount, len(wordDict.keys())))\n",
    "\n",
    "for i, vol in enumerate(volumes):\n",
    "    tok_list = vol.tokenlist(pages=False)\n",
    "    counts = list(tok_list['count'])\n",
    "    tokens = tok_list.index.get_level_values('token')\n",
    "    \n",
    "    for token, count in zip(tokens, counts):\n",
    "        try:\n",
    "            index = wordDict[token]\n",
    "            dtm[i, index] = count\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "X = dtm\n",
    "y = np.zeros((400))\n",
    "y[200:400] = 1\n",
    "\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use the `TfidfTransformer` to format the bag of words matrix, so that we can fit it to our LinearSVC model. Let's see how our model does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chench/anaconda/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8975\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import cross_validation\n",
    "\n",
    "tfidf = TfidfTransformer()\n",
    "out = tfidf.fit_transform(X, y)\n",
    "\n",
    "model = LinearSVC()\n",
    "\n",
    "score = cross_validation.cross_val_score(model, X, y, cv=10)\n",
    "print(np.mean(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get the most helpful features, or words, for each class. First we'll `fit` the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'w'),\n",
       " (2, 'science'),\n",
       " (3, 'fiction'),\n",
       " (4, 'SF'),\n",
       " (5, '\"'),\n",
       " (6, 'Science'),\n",
       " (7, 'Fiction'),\n",
       " (8, 'London'),\n",
       " (9, 'from'),\n",
       " (10, 'at'),\n",
       " (11, 'you'),\n",
       " (12, 'story'),\n",
       " (13, 'it'),\n",
       " (14, 'into'),\n",
       " (15, '?'),\n",
       " (16, 'are'),\n",
       " (17, ':'),\n",
       " (18, 'Earth'),\n",
       " (19, 'But'),\n",
       " (20, 'they'),\n",
       " (21, 'He'),\n",
       " (22, 'stories'),\n",
       " (23, 'space'),\n",
       " (24, '('),\n",
       " (25, 'aliens'),\n",
       " (26, 'an'),\n",
       " (27, 'he'),\n",
       " (28, 'has'),\n",
       " (29, 'I'),\n",
       " (30, 'me'),\n",
       " (31, 'ale'),\n",
       " (32, 'but'),\n",
       " (33, 'W'),\n",
       " (34, 'two'),\n",
       " (35, 'Director'),\n",
       " (36, 'have'),\n",
       " (37, 'lub'),\n",
       " (38, 'za'),\n",
       " (39, 'could'),\n",
       " (40, 'Wells'),\n",
       " (41, 'your'),\n",
       " (42, 'Roberts'),\n",
       " (43, 'ship'),\n",
       " (44, 'only'),\n",
       " (45, 'do'),\n",
       " (46, 'fantasy'),\n",
       " (47, 'time'),\n",
       " (48, 'planet'),\n",
       " (49, 'tak'),\n",
       " (50, 'są')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats = np.argsort(model.coef_[0])[:50]\n",
    "top_scifi = [(list(feats).index(wordDict[w]) + 1, w) for w in wordDict.keys() if wordDict[w] in feats]\n",
    "sorted(top_scifi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'deal'),\n",
       " (2, 'AND'),\n",
       " (3, 'whole'),\n",
       " (4, 'Land'),\n",
       " (5, '10'),\n",
       " (6, 'America'),\n",
       " (7, 'Master'),\n",
       " (8, 'Columbia'),\n",
       " (9, 'live'),\n",
       " (10, 'after'),\n",
       " (11, 'on'),\n",
       " (12, 'rule'),\n",
       " (13, 'On'),\n",
       " (14, 'by'),\n",
       " (15, 'White'),\n",
       " (16, 'H.'),\n",
       " (17, \"'\"),\n",
       " (18, 'which'),\n",
       " (19, 'or'),\n",
       " (20, 'country'),\n",
       " (21, \"'s\"),\n",
       " (22, 'that'),\n",
       " (23, 'this'),\n",
       " (24, 'American'),\n",
       " (25, 'A'),\n",
       " (26, 'not'),\n",
       " (27, 'her'),\n",
       " (28, 'came'),\n",
       " (29, 'poetry'),\n",
       " (30, 'For'),\n",
       " (31, 'By'),\n",
       " (32, 'love'),\n",
       " (33, 'Law'),\n",
       " (34, 'planted'),\n",
       " (35, 'To'),\n",
       " (36, '1'),\n",
       " (37, 'fall'),\n",
       " (38, 'let'),\n",
       " (39, 'History'),\n",
       " (40, ';'),\n",
       " (41, 'truth'),\n",
       " (42, 'free'),\n",
       " (43, 'The'),\n",
       " (44, \"'ll\"),\n",
       " (45, 'all'),\n",
       " (46, 'we'),\n",
       " (47, 'Our'),\n",
       " (48, 'land'),\n",
       " (49, 'our'),\n",
       " (50, 'We')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats = np.argsort(model.coef_[0])[-50:]\n",
    "top_poetry = [(list(feats).index(wordDict[w]) + 1, w) for w in wordDict.keys() if wordDict[w] in feats]\n",
    "sorted(top_poetry, key=lambda tup: tup[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Author Gender Classification\n",
    "\n",
    "Our next task will be to classify text based on the author's gender. We can find this under the 'htrc_gender' attribute found in each volume's metadata.\n",
    "\n",
    "We will create the training set using the existing volumes we have already seen in the previous example by searching the metadata fields for gender. We will then add the volumes with these attributes, as well as add the correct labels to our `y` vector. For this example, we will use 10 training samples for each class to show the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vols = []\n",
    "y_2 = []\n",
    "\n",
    "wordDictGender = {}\n",
    "subjects = [history, poetry, scifi]\n",
    "index = 0\n",
    "male = 0\n",
    "female = 0\n",
    "\n",
    "for subject in subjects:\n",
    "    for vol in subject.volumes():\n",
    "        if male == 10 and female == 10:\n",
    "            break\n",
    "        try:\n",
    "            if str(vol.metadata['htrc_gender'][0]) == 'male':\n",
    "                if male < 10:\n",
    "                    vols.append(vol)\n",
    "                    tok_list = vol.tokenlist(pages=False)\n",
    "                    tokens = tok_list.index.get_level_values('token')\n",
    "                    for token in tokens:\n",
    "                        if token not in wordDictGender.keys():\n",
    "                            wordDictGender[token] = index\n",
    "                            index += 1\n",
    "                            \n",
    "                    y_2.append(0)\n",
    "                    male += 1\n",
    "                    \n",
    "            elif str(vol.metadata['htrc_gender'][0]) == 'female':\n",
    "                if female < 10:\n",
    "                    vols.append(vol)\n",
    "                    tok_list = vol.tokenlist(pages=False)\n",
    "                    tokens = tok_list.index.get_level_values('token')\n",
    "                    \n",
    "                    for token in tokens:\n",
    "                        if token not in wordDictGender.keys():\n",
    "                            wordDictGender[token] = index\n",
    "                            index += 1\n",
    "                    \n",
    "                    y_2.append(1)\n",
    "                    female += 1\n",
    "        except:\n",
    "            pass\n",
    "    if male == 10 and female == 10:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then take these volumes and create our bag of words matrix as we did in the previous example. Finally, we can run our LinearSVC model once again to show how well our model does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating bag of words matrix\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"Generating bag of words matrix\")\n",
    "volCount = 20\n",
    "dtm_gender = np.zeros((volCount, len(wordDictGender.keys())))\n",
    "\n",
    "for i, vol in enumerate(vols):\n",
    "    tok_list = vol.tokenlist(pages=False)\n",
    "    counts = list(tok_list['count'])\n",
    "    tokens = tok_list.index.get_level_values('token')\n",
    "    \n",
    "    for token, count in zip(tokens, counts):\n",
    "        try:\n",
    "            index = wordDictGender[token]\n",
    "            dtm_gender[i, index] = count\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "X_2 = dtm_gender\n",
    "\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexchan/anaconda3/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.85\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_validation\n",
    "\n",
    "tfidf = TfidfTransformer()\n",
    "out = tfidf.fit_transform(X_2, y_2)\n",
    "\n",
    "model = LinearSVC()\n",
    "\n",
    "score = cross_validation.cross_val_score(model, X_2, y_2, cv=10, random_state=40)\n",
    "print(np.mean(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(X_2, y_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feats = np.argsort(model.coef_[0])[:50]\n",
    "top_male = [(list(feats).index(wordDict[w]) + 1, w) for w in wordDict.keys() if wordDict[w] in feats]\n",
    "sorted(top_male)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feats = np.argsort(model.coef_[0])[-50:]\n",
    "top_female = [(list(feats).index(wordDict[w]) + 1, w) for w in wordDict.keys() if wordDict[w] in feats]\n",
    "sorted(top_female, key=lambda tup: tup[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Congratulations! You've finished the tutorial. You now have the tools to run your own classification tasks with the HTRC library. Try using different models or adding more volumes to increase your accuracy scores for your model."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
