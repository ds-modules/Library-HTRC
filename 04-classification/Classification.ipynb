{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "This notebook demonstrates some of the classification tasks that can be accomplished using data retrieved from the HTRC API.\n",
    "\n",
    "We will be using the Scikit Learn library to tackle these classification problems. Install the library using pip:\n",
    "```\n",
    "   pip install sklearn\n",
    "```\n",
    "\n",
    "For these examples, we will be gathering data using the advance search from the Hathi Trust library. To create your training set, determine a search query that will become your labels. Once you search, add the search results to a collection named after your label.\n",
    "\n",
    "Once you have completed adding to your collection, go to \"My Collections\" under ____. There you will find a button called \"Download Metadata\".\n",
    "\n",
    "Download the JSON file associated with your collection and place it in the local directory that you are working in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "jsonFiles = [file for file in os.listdir('.') if file.find('json') != -1]\n",
    "\n",
    "txts = []\n",
    "for file in jsonFiles:\n",
    "    with open(file) as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "    texts = data['gathers']\n",
    "    ids = [text['htitem_id'] for text in texts]\n",
    "    \n",
    "    filename = data['title'] + '.txt'\n",
    "    txts.append(filename)\n",
    "    \n",
    "    #write each id into txt file\n",
    "    with open(filename, 'w') as f:\n",
    "        for textid in ids:\n",
    "            f.write(textid + '\\n')\n",
    "\n",
    "print(\"JSON files created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this step is complete, you can follow the instructions in the Setup notebook to load the data as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Genre Classification\n",
    "\n",
    "In this example, we'll be classifying texts into 3 different genres: Poetry, History, and Science Fiction. JSON files containing the metadata for 500 texts in each genre have been included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "history_output = !htid2rsync --f history.txt | rsync -azv --files-from=- data.sharc.hathitrust.org::features/ history/\n",
    "poetry_output = !htid2rsync --f poetry.txt | rsync -azv --files-from=- data.sharc.hathitrust.org::features/ poetry/\n",
    "scifi_output = !htid2rsync --f scifi.txt | rsync -azv --files-from=- data.sharc.hathitrust.org::features/ scifi/\n",
    "outputs = list([history_output, poetry_output, scifi_output])\n",
    "subjects = ['history', 'poetry', 'scifi']\n",
    "\n",
    "paths = {}\n",
    "suffix = '.json.bz2'\n",
    "for subject, output in zip(subjects, outputs):\n",
    "    folder = subject\n",
    "    filePaths = [path for path in output if path.endswith(suffix)]\n",
    "    paths[subject] = [os.path.join(folder, path) for path in filePaths]\n",
    "    fn = subject + '_paths.txt'\n",
    "    with open(fn, 'w') as f:\n",
    "        for path in paths[subject]:\n",
    "            p = str(path) + '\\n'\n",
    "            f.write(p)\n",
    "\n",
    "#write paths into a .txt file\n",
    "\n",
    "print(\"Path extraction complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save volumes for each class from their respective paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished reading paths\n"
     ]
    }
   ],
   "source": [
    "from htrc_features import FeatureReader\n",
    "\n",
    "paths = {}\n",
    "subjects = ['history', 'poetry', 'scifi']\n",
    "for subject in subjects:\n",
    "    with open(subject + '_paths.txt', 'r') as f:\n",
    "#         print(f.readlines())\n",
    "        paths[subject] = [line[:len(line)-1] for line in f.readlines()]\n",
    "        \n",
    "history = FeatureReader(paths['history'])\n",
    "\n",
    "poetry = FeatureReader(paths['poetry'])\n",
    "\n",
    "scifi = FeatureReader(paths['scifi'])\n",
    "\n",
    "print(\"Finished reading paths\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now create a string consisting of all the tokens in the book. This string can then easily be fed into Scikit-Learn's CountVectorizer, which will create the bag of words model for all of the volumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "def isPure(s):\n",
    "    return not any(char.isdigit() for char in s) and re.match('^[\\w-]+$', s) is not None\n",
    "\n",
    "def tableToString(ts, cs):\n",
    "    s = \"\"\n",
    "    for token, count in zip(ts, cs):\n",
    "        if isPure(token):\n",
    "            for i in cs:\n",
    "                s += token + \" \"\n",
    "    return s\n",
    "\n",
    "def getTokenString(vol):\n",
    "    volText = \"\"\n",
    "    for page in vol:\n",
    "        if page.token_count() > 0:\n",
    "            counts = [t for t in page.tokenlist()['count']]\n",
    "            tokens = page.tokens()\n",
    "            volString = tableToString(tokens, counts)\n",
    "            volText += volString\n",
    "    return volText    \n",
    "\n",
    "def writeVols(volumes, dirname, numTexts):\n",
    "    if not os.path.exists(dirname):\n",
    "        os.mkdir(dirname)\n",
    "    i = 0\n",
    "    for vol in volumes:\n",
    "        if i == numTexts:\n",
    "            break\n",
    "        i += 1\n",
    "        print(i, vol.title)\n",
    "        volText = getTokenString(vol)\n",
    "        fn = dirname + '/' + str(i) + '.txt'\n",
    "        with open(fn, 'w') as f:\n",
    "            f.write(volText)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scifiDir = 'scifi_texts'\n",
    "histDir = 'history_texts'\n",
    "\n",
    "writeTokenString(scifi.volumes(), scifiDir, 10)\n",
    "writeTokenString(history.volumes(), histDir, 10)\n",
    "\n",
    "print('Finished processing texts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now generate our training sets by reading in the text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated training data\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "for file in os.listdir('scifi_texts'):\n",
    "    with open('scifi_texts/' + file) as f:\n",
    "        X.append(str(f.readlines()))\n",
    "        y.append(0)\n",
    "\n",
    "for file in os.listdir('hist_texts'):\n",
    "    with open('hist_texts/' + file) as f:\n",
    "        X.append(str(f.readlines()))\n",
    "        y.append(1)\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "X, y = shuffle(X, y, random_state=0)\n",
    "print(\"Generated training data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our training set, we can use scikit-learn to generate our bag of words model and test how well it does using a support vector classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.          0.83333333  1.        ] 0.944444444444\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import cross_validation\n",
    "\n",
    "text_clf_genre = Pipeline([('vect', CountVectorizer(ngram_range=(1, 2))),\n",
    "                    ('tfidf', TfidfTransformer()),\n",
    "                    ('clf', LinearSVC(random_state=0))\n",
    "                     ])\n",
    "scores = cross_validation.cross_val_score(text_clf_genre, X, y)\n",
    "print(scores, np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Author Gender Classification\n",
    "\n",
    "Our next task will be to classify text based on the author's gender. We can find this under the 'htrc_gender' attribute found in each volume's metadata.\n",
    "\n",
    "We will create the training set using the existing volumes we have already seen in the previous example by searching the metadata fields for gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_2 = []\n",
    "y_2 = []\n",
    "\n",
    "if not os.path.exists('female_texts'):\n",
    "    os.mkdir('female_texts')\n",
    "\n",
    "if not os.path.exists('male_texts'):\n",
    "    os.mkdir('male_texts')\n",
    "\n",
    "subjects = [history, poetry, scifi]\n",
    "male = 0\n",
    "female = 0\n",
    "for subject in subjects:\n",
    "    for vol in subject.volumes():\n",
    "        if male == 10 and female == 10:\n",
    "            break\n",
    "        try:\n",
    "            if vol.metadata['htrc_gender'][0] == 'male':\n",
    "                if male < 10:\n",
    "                    X2.append(getTokenString(vol))\n",
    "                    y2.append(0)\n",
    "                    male += 1\n",
    "            else:\n",
    "                if female < 10:\n",
    "                    X2.append(getTokenString(vol))\n",
    "                    y2.append(1)\n",
    "                    female += 1\n",
    "        except:\n",
    "            pass\n",
    "    if male == 10 and female == 10:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_clf_gender = Pipeline([('vect', CountVectorizer(ngram_range=(1, 2))),\n",
    "                    ('tfidf', TfidfTransformer()),\n",
    "                    ('clf', LinearSVC(random_state=0))\n",
    "                     ])\n",
    "scores_gender = cross_validation.cross_val_score(text_clf_gender, X_2, y_2)\n",
    "print(scores_gender, np.mean(scores_gender))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
