{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from htrc_features import FeatureReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genre classification with HTRC data\n",
    "\n",
    "In this example, we'll be classifying texts into 2 different genres: poetry and science-fiction. JSON files containing the metadata for 100 texts in each genre need to be downloaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "poetry_output = !htid2rsync --f data/poetry.txt | rsync -azv --files-from=- data.sharc.hathitrust.org::features/ data/poetry/\n",
    "scifi_output = !htid2rsync --f data/scifi.txt | rsync -azv --files-from=- data.sharc.hathitrust.org::features/ data/scifi/\n",
    "\n",
    "outputs = list([poetry_output, scifi_output])\n",
    "subjects = ['poetry', 'scifi']\n",
    "\n",
    "paths = {}\n",
    "suffix = '.json.bz2'\n",
    "for subject, output in zip(subjects, outputs):\n",
    "    folder = subject\n",
    "    filePaths = [path for path in output if path.endswith(suffix)]\n",
    "    paths[subject] = [os.path.join(folder, path) for path in filePaths]\n",
    "    fn = 'data/' + subject + '_paths.txt'\n",
    "    with open(fn, 'w') as f:\n",
    "        for path in paths[subject]:\n",
    "            p = str(path) + '\\n'\n",
    "            f.write(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous notebooks, we'll construct `FeatureReader` objects for each corpus. The line below reads in path files we created to the downloaded data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = {}\n",
    "subjects = ['poetry', 'scifi']\n",
    "for subject in subjects:\n",
    "    with open('data/' + subject + '_paths.txt', 'r') as f:\n",
    "        paths[subject] = ['data/' + line[:len(line)-1] for line in f.readlines()]\n",
    "        \n",
    "poetry = FeatureReader(paths['poetry'])\n",
    "scifi = FeatureReader(paths['scifi'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create our bag of words matrix, we need to keep a global dictionary of all words seen in each of our texts. We initialize \"wordDict\", which tracks all the words seen and records its index in the bag of words matrix. We also keep a list of volumes so that we can parse them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createWordDict(HTRC_FeatureReader_List):\n",
    "\n",
    "    wordDict = {}\n",
    "    i = 0 \n",
    "    volumes = []\n",
    "\n",
    "    for f in HTRC_FeatureReader_List:\n",
    "\n",
    "        for vol in f.volumes():\n",
    "            \n",
    "            volumes.append(vol)\n",
    "\n",
    "            tok_list = vol.tokenlist(pages=False)\n",
    "            tokens = tok_list.index.get_level_values('token')\n",
    "\n",
    "            for token in tokens:\n",
    "                if token not in wordDict.keys():\n",
    "                    wordDict[token] = i\n",
    "                    i += 1\n",
    "    \n",
    "    return wordDict, volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chench/anaconda/lib/python3.5/site-packages/htrc_features/feature_reader.py:603: FutureWarning: sortlevel is deprecated, use sort_index(level= ...)\n",
      "  df.sortlevel(inplace=True)\n"
     ]
    }
   ],
   "source": [
    "wordDict, volumes = createWordDict([scifi, poetry])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we construct the global dictionary, we can fill the bag of words matrix with the word counts for each volume. Once we have this, we will use it to format the training data for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm = np.zeros((200, len(wordDict.keys())))\n",
    "\n",
    "for i, vol in enumerate(volumes):\n",
    "    tok_list = vol.tokenlist(pages=False)\n",
    "    counts = list(tok_list['count'])\n",
    "    tokens = tok_list.index.get_level_values('token')\n",
    "    \n",
    "    for token, count in zip(tokens, counts):\n",
    "        try:\n",
    "            index = wordDict[token]\n",
    "            dtm[i, index] = count\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "X = dtm\n",
    "y = np.zeros((200))\n",
    "y[100:200] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use the `TfidfTransformer` to format the bag of words matrix, so that we can fit it to our LinearSVC model. Let's see how our model does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chench/anaconda/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.915\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import cross_validation\n",
    "\n",
    "tfidf = TfidfTransformer()\n",
    "out = tfidf.fit_transform(X, y)\n",
    "\n",
    "model = LinearSVC()\n",
    "\n",
    "score = cross_validation.cross_val_score(model, X, y, cv=10)\n",
    "print(np.mean(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get the most helpful features, or words, for each class. First we'll `fit` the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, '\"'),\n",
       " (2, 'science'),\n",
       " (3, 'fiction'),\n",
       " (4, \"n't\"),\n",
       " (5, '('),\n",
       " (6, 'it'),\n",
       " (7, 'do'),\n",
       " (8, 'und'),\n",
       " (9, 'is'),\n",
       " (10, 'w'),\n",
       " (11, ')'),\n",
       " (12, 'they'),\n",
       " (13, 'I'),\n",
       " (14, 'she'),\n",
       " (15, 'may'),\n",
       " (16, 'into'),\n",
       " (17, 'was'),\n",
       " (18, ','),\n",
       " (19, 'would'),\n",
       " (20, 'could'),\n",
       " (21, 'between'),\n",
       " (22, 'story'),\n",
       " (23, 'space'),\n",
       " (24, \"'s\"),\n",
       " (25, 'them'),\n",
       " (26, 'which'),\n",
       " (27, 'W'),\n",
       " (28, 'Fiction'),\n",
       " (29, 'what'),\n",
       " (30, 'Wells'),\n",
       " (31, 'You'),\n",
       " (32, 'nature'),\n",
       " (33, 'her'),\n",
       " (34, 'Obsada'),\n",
       " (35, 'Produkcja'),\n",
       " (36, 'Zdjęcia'),\n",
       " (37, 'Montaż'),\n",
       " (38, 'o'),\n",
       " (39, 'Muzyka'),\n",
       " (40, 'specjalne'),\n",
       " (41, 'He'),\n",
       " (42, 'or'),\n",
       " (43, 'p'),\n",
       " (44, 'novel'),\n",
       " (45, \"'ll\"),\n",
       " (46, 'other'),\n",
       " (47, 'at'),\n",
       " (48, 'world'),\n",
       " (49, 'now'),\n",
       " (50, 'himself')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats = np.argsort(model.coef_[0])[:50]\n",
    "top_scifi = [(list(feats).index(wordDict[w]) + 1, w) for w in wordDict.keys() if wordDict[w] in feats]\n",
    "sorted(top_scifi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'While'),\n",
       " (2, 'r'),\n",
       " (3, '“'),\n",
       " (4, 'V'),\n",
       " (5, 'P'),\n",
       " (6, 'fond'),\n",
       " (7, 'ev'),\n",
       " (8, 'yearning'),\n",
       " (9, 'camp-fires'),\n",
       " (10, '¿'),\n",
       " (11, 'lhc'),\n",
       " (12, 'Г'),\n",
       " (13, 'praying'),\n",
       " (14, 'soldier'),\n",
       " (15, 'n'),\n",
       " (16, 'burning'),\n",
       " (17, 's'),\n",
       " (18, 'His'),\n",
       " (19, 'life'),\n",
       " (20, 'America'),\n",
       " (21, 'By'),\n",
       " (22, 'your'),\n",
       " (23, 'New'),\n",
       " (24, 'my'),\n",
       " (25, 'and'),\n",
       " (26, 'And'),\n",
       " (27, 'for'),\n",
       " (28, 'f'),\n",
       " (29, 'J'),\n",
       " (30, 'dear'),\n",
       " (31, 'er'),\n",
       " (32, 'with'),\n",
       " (33, 'his'),\n",
       " (34, 'on'),\n",
       " (35, ':'),\n",
       " (36, '1'),\n",
       " (37, 'l'),\n",
       " (38, 'hearts'),\n",
       " (39, '—'),\n",
       " (40, \"'\"),\n",
       " (41, 'our'),\n",
       " (42, 'will'),\n",
       " (43, 'by'),\n",
       " (44, 'The'),\n",
       " (45, 'v'),\n",
       " (46, 'Brother'),\n",
       " (47, 'in'),\n",
       " (48, ';'),\n",
       " (49, '`'),\n",
       " (50, '-')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats = np.argsort(model.coef_[0])[-50:]\n",
    "top_poetry = [(list(feats).index(wordDict[w]) + 1, w) for w in wordDict.keys() if wordDict[w] in feats]\n",
    "sorted(top_poetry, key=lambda tup: tup[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
