{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing volumes for word frequencies\n",
    "This notebook will demonstrate some of basic functionality of the Hathi Trust FeatureReader object. We will look at a few examples of easily replicable text analysis techniques — namely word frequency and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from htrc_features import FeatureReader\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 — Word frequency in novels\n",
    "The following cells load a collection of nine novels from the 18th-20th centuries, chosen from an HTRC collection. Also loaded are a collection of math textbooks from the 17th-19th centuries, but the latter will be used in a later part. The collection of novels will be used as a departure point for our text analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!rm -rf local-folder\n",
    "download_output = !htid2rsync --f novels-word-use.txt | rsync -azv --files-from=- data.sharc.hathitrust.org::features/ local-folder/\n",
    "suffix = '.json.bz2'\n",
    "file_paths = ['local-folder/' + path for path in download_output if path.endswith(suffix)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fr_novels = FeatureReader(file_paths)\n",
    "for vol in fr_novels:\n",
    "    print(vol.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting volumes\n",
    "The following cell is useful in choosing a volume to manipulate. Set `title_word` to any word that is contained in the title of the fr-volume you would like to work with (the string comparison is case-insensitive since some titles are lower-case). The volume will then be stored as 'vol', and can be reassigned to any variable name you would like! As an example, `title_word` is currently set to \"grapes\", meaning \"The Grapes of Wrath\" by John Steinbeck is the current volume saved under the variable name vol. You can change this cell at any time to work with a different volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "title_word = 'grapes'\n",
    "\n",
    "for vol in fr_novels:\n",
    "    if title_word.lower() in vol.title.lower():\n",
    "        print('Current volume:', vol.title)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling tokens from a book\n",
    "The following cell will display the most common tokens (words or punctuation marks) in a given volume, alongside the number of times they appear. It will also calculate their relative frequencies (found by dividing the number of appearances over the total number of words in the book) and display the results in a `DataFrame`. We'll do this for the volume we found above, the cell may take a few seonds to run because we're looping through every word in the volume!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokens = vol.tokenlist(pos=False, case=False, pages=False).sort_values('count', ascending=False)\n",
    "\n",
    "freqs = []\n",
    "for count in tokens['count']:\n",
    "    freqs.append(count/sum(tokens['count']))\n",
    "    \n",
    "tokens['rel_frequency'] = freqs\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphing word frequencies\n",
    "The following cell outputs a bar plot of the most common tokens from the volume and their frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Build a list of frequencies and a list of tokens.\n",
    "freqs_1, tokens_1 = [], []\n",
    "for i in range(15):  # top 8 words\n",
    "    freqs_1.append(freqs[i])\n",
    "    tokens_1.append(tokens.index.get_level_values('lowercase')[i])\n",
    "\n",
    "# Create a range for the x-axis\n",
    "x_ticks = np.arange(len(tokens_1))\n",
    "\n",
    "# Plot!\n",
    "plt.bar(x_ticks, freqs_1)\n",
    "plt.xticks(x_ticks, tokens_1)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "plt.xlabel('Token', fontsize=14)\n",
    "plt.title('Common token frequencies in \"' + vol.title[:14] + '...\"', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the most common tokens in \"The Grapes of Wrath\" are mostly punctuation and basic words that don't provide context. Let's see if we can narrow our search to gain some more relevant insight. We can get a list of stopwords from the `nltk` library. Punctuation is in the `string` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "print(stopwords.words('english'))\n",
    "\n",
    "print()\n",
    "\n",
    "print(punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a list of words to ignore in our search, we can make a few tweaks to our plotting cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freqs_filtered, tokens_filtered, i = [], [], 0\n",
    "while len(tokens_filtered) < 15:\n",
    "    if tokens.index.get_level_values('lowercase')[i] not in stopwords.words('english') + list(punctuation):\n",
    "        freqs_filtered.append(freqs[i])\n",
    "        tokens_filtered.append(tokens.index.get_level_values('lowercase')[i])\n",
    "    i += 1\n",
    "\n",
    "# Create a range for the x-axis\n",
    "x_ticks = np.arange(len(freqs_filtered))\n",
    "\n",
    "# Plot!\n",
    "plt.bar(x_ticks, freqs_filtered)\n",
    "plt.xticks(x_ticks, tokens_filtered)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "plt.xlabel('Token', fontsize=14)\n",
    "plt.title('Common token frequencies in \"' + vol.title[:14] + '...\"', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's better. No more punctuation and lower frequencies on the y-axis mean that narrowing down our search choices was effective. This is also helpful if we're trying to find distinctive words in a text, because we removed the words that most texts share."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling tokens from all books\n",
    "Now we can see how relative word frequencies compare across all the books in our sample. To do this, we'll need a few useful functions.\n",
    "The first finds the most common noun in a volume, with adjustable parameters for minimum length.\n",
    "The second calculates the relative frequency of a token across the entirety of a volume, saving us the time of doing the calculation like in the above cell.\n",
    "Finally, we'll have a visualization function to create a bar plot of relative frequencies for all volumes in our sample, so that we can easily track how word frequencies differ across titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A function to return the most common noun of length at least word_length in the volume.\n",
    "# NOTE: word_length defaults to 2.\n",
    "# e.g. most_common_noun(fr_novels.first) returns 'time'.\n",
    "\n",
    "def most_common_noun(vol, word_length=2):   \n",
    "    # Build a table of common nouns\n",
    "    tokens_1 = vol.tokenlist(pages=False, case=False)\n",
    "    nouns_only = tokens_1.loc[(slice(None), slice(None), ['NN']),]\n",
    "    top_nouns = nouns_only.sort_values('count', ascending=False)\n",
    "\n",
    "    token_index = top_nouns.index.get_level_values('lowercase')\n",
    "    \n",
    "    # Choose the first token at least as long as word_length with non-alphabetical characters\n",
    "    for i in range(max(token_index.shape)):\n",
    "        if (len(token_index[i]) >= word_length):\n",
    "            if(\"'\", \"!\", \",\", \"?\" not in token_index[i]):\n",
    "                return token_index[i]\n",
    "    print('There is no noun of this length')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "most_common_noun(vol, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Return the usage frequency of a given word in a given volume. \n",
    "# NOTE: frequency() returns a dictionary entry of the form {'word': frequency}.\n",
    "# e.g. frequency(fr_novels.first(), 'blue') returns {'blue': 0.00012}\n",
    "\n",
    "def frequency(vol, word):\n",
    "    t1 = vol.tokenlist(pages=False, pos=False, case=False)\n",
    "    token_index = t1[t1.index.get_level_values(\"lowercase\") == word]\n",
    "    \n",
    "    if len(token_index['count'])==0:\n",
    "        return {word: 0}\n",
    "    \n",
    "    count = token_index['count'][0]\n",
    "    freq = count/sum(t1['count'])\n",
    "    \n",
    "    return {word: float('%.5f' % freq)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "frequency(vol, 'blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Returns a plot of the usage frequencies of the given word across all volumes in the given FeatureReader collection.\n",
    "# NOTE: frequencies are given as percentages rather than true ratios.\n",
    "def frequency_bar_plot(word, fr):\n",
    "    freqs, titles = [], []\n",
    "    for vol in fr:\n",
    "        title = vol.title\n",
    "        short_title = title[:6] + (title[6:] and '..')\n",
    "        freqs.append(100*frequency(vol, word)[word])\n",
    "        titles.append(short_title)\n",
    "        \n",
    "    # Format and plot the data\n",
    "    x_ticks = np.arange(len(titles))\n",
    "    plt.bar(x_ticks, freqs)\n",
    "    plt.xticks(x_ticks, titles, fontsize=10)\n",
    "    plt.ylabel('Frequency (%)', fontsize=12)\n",
    "    plt.title('Frequency of \"' + word + '\"', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "frequency_bar_plot('blue', fr_novels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your turn! See if you can output a bar plot of the most common noun of length at least 5 in \"To Kill a Mockingbird\". REMEMBER, you may have to set vol to a different value than it already has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use the provided frequency functions to plot the most common 5-letter noun in \"To Kill a Mockinbird\".\n",
    "# Your solution should be just one line of code.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2— Non-fiction volumes\n",
    "Now we'll load a collection of 33 math textbooks from the 18th and 19th centuries. These volumes focus on number theory and arithmetic, and were written during the lives of Leonhard Euler and Joseph-Louis Lagrange – two of the most prolific researchers of number theory in all of history. As a result, we can expect the frequency of certain words and topics to shift over time to reflect the state of contemporary research. Let's load them and see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "download_output = !htid2rsync --f math-collection.txt | rsync -azv --files-from=- data.sharc.hathitrust.org::features/ local-folder/\n",
    "file_paths = ['local-folder/' + path for path in download_output if path.endswith(suffix)]\n",
    "fr_math = FeatureReader(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fr_math = FeatureReader(file_paths)\n",
    "for vol in fr_math:\n",
    "    print(vol.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another frequency function\n",
    "The next cell contains a frequency_by_year function that takes as inputs a query word and a FeatureReader object. The function calculates relative frequencies of the query word across all volumes in the FR, then outputs them in a `DataFrame` sorted by the volume year. It then plots the frequencies and allows us to easily see trends in word usage across a time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Returns a DF of relative frequencies, volume years, and page counts, along with a scatter plot.\n",
    "# NOTE: frequencies are given in percentages rather than true ratios.\n",
    "def frequency_by_year(query_word, fr):\n",
    "    volumes = pd.DataFrame()\n",
    "    years, page_counts, query_freqs = [], [], []\n",
    "\n",
    "    for vol in fr:\n",
    "        years.append(int(vol.year))\n",
    "        page_counts.append(int(vol.page_count))\n",
    "        query_freqs.append(100*frequency(vol, query_word)[query_word])\n",
    "    \n",
    "    volumes['year'], volumes['pages'], volumes['freq'] = years, page_counts, query_freqs\n",
    "    volumes = volumes.sort_values('year')\n",
    "    \n",
    "    # Set plot dimensions and labels\n",
    "    scatter_plot = volumes.plot.scatter('year', 'freq', color='black', s=50, fontsize=12)\n",
    "    plt.ylim(0-np.mean(query_freqs), max(query_freqs)+np.mean(query_freqs))\n",
    "    plt.ylabel('Frequency (%)', fontsize=12)\n",
    "    plt.xlabel('Year', fontsize=12)\n",
    "    plt.title('Frequency of \"' + query_word + '\"', fontsize=14)\n",
    "    \n",
    "    return volumes.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for shifts over time\n",
    "In 1744, Euler began a huge volume of work on identifying quadratic forms and progressions of primes. It follows from reason, then, that the mentions of these topics in number theory textbooks should see a discernible jump following the 1740's. The following cells call frequency_by_year on several relevant words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "frequency_by_year('quadratic', fr_math)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "frequency_by_year('prime', fr_math)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "frequency_by_year('factor', fr_math)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All done!\n",
    "That's all for this notebook, but it doesn't mean you can't apply what you've learned. Can you think of any words you'd like to track over time? Feel free to use the following empty cells however you'd like. An interesting challenge would be to see if you can incorporate the frequency functions from Part 1 into the scatter function from Part 2. Have fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
