{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HathiTrust Research Center (HTRC)\n",
    "\n",
    "The [HathiTrust Digital Library](https://www.hathitrust.org/) contains over 14 million volumes scanned from academic libraries around the world (primarily in North America). The [HathiTrust Research Center](https://analytics.hathitrust.org/) allows researchers to access almost all of those texts in a few different modes for computational text analysis. \n",
    "\n",
    "This notebook will walk us through getting set-up to analyze [HTRC Extracted Features](https://wiki.htrc.illinois.edu/display/COM/Extracted+Features+Dataset) for volumes in HathiTrust in a Jupyter/Python environment. *Extracted Features* are currently (as of April 2017) the most robust way to access in-copyright works from the HT Library for computational analysis. \n",
    "\n",
    "For more information on HTRC: \n",
    "* [Library text mining guide page on HTRC](http://guides.lib.berkeley.edu/c.php?g=491766&p=3381443)\n",
    "* [Programming Historian's Text Mining in Python through the HTRC Feature Reader](http://programminghistorian.org/lessons/text-mining-with-extracted-features)\n",
    "\n",
    "## Installation\n",
    "\n",
    "To start we'll need to install a few things:\n",
    "* Install the *HTRC Feature Reader* to work with Extracted Features: \n",
    "```\n",
    "conda install -c htrc htrc-feature-reader\n",
    "``` \n",
    "or\n",
    "```\n",
    "pip install htrc-feature-reader\n",
    "pip install matplotlib jupyter\n",
    "```\n",
    "* Install Rsync to download Extracted Features from HathiTrust:\n",
    "\n",
    "  * For Linux:\n",
    "```\n",
    "yum -y install rsync\n",
    "```\n",
    "  * For Mac you need to use [Homebrew](https://brew.sh/) to install Rsync. To install Homebrew:\n",
    "```\n",
    "/usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"\n",
    "```\n",
    "  * And then to install Rsync on Mac using Homebrew:\n",
    "```\n",
    "brew tap homebrew/dupes\n",
    "brew install rsync\n",
    "```\n",
    "\n",
    "## Adding volumes from HathiTrust\n",
    "\n",
    "To build your own corpus, you first need to find the volumes you'd like to include in the [HathiTrust Library](https://www.hathitrust.org/). Alternately, you can access volumes from existing [public HT collections](https://babel.hathitrust.org/cgi/mb?colltype=featured), or use one of the sample datasets included below under the *Sample datasets* heading. To access extracted features from HathiTrust:\n",
    "\n",
    "* Install and configure [the HT + HTRC mashup](https://data.analytics.hathitrust.org/features/) browser extension.\n",
    "* Once the extension is running, go to the [HathiTrust Library](https://www.hathitrust.org/), and search for the titles you want to include.\n",
    "* You can manually download extracted features one result at a time by simply choosing the *Download Extracted Features* link for any item in your search results. Save the .json.bz2 file or files and skip to the next section, *Working with Extracted Features* below to load them into your workspace.\n",
    "* If you plan to work with a large number of texts, you might choose instead to create a collection in HathiTrust, and then download the Extracted Features for the entire collection at once. This requires a valid CalNet ID. \n",
    "\n",
    "### To create a collection:\n",
    "\n",
    "* [Login to HathiTrust](https://www.hathitrust.org/shibboleth)\n",
    "* Change the HathiTrust search tab to *Full-Text* or go to the [Advanced Full-Text search](https://babel.hathitrust.org/cgi/ls?a=page;page=advanced).\n",
    "* Check the boxes to the left of any search results you want to add to your collection (or select all), and use the *Select Collection* dropdown to *Add Selected* volumes to collections of your own design.\n",
    "* Choose *My Collections* from the top of the HathiTrust interface, choose your collection, and from the *Download Metadata* button/dropdown choose the TSV option.\n",
    "* Open the TSV file, and then delete all of the columns except for the first column, *htitem_id.* Delete the *htitem_id* header row as well and then save the file to your working directory.\n",
    "\n",
    "## Loading Extracted Features\n",
    "\n",
    "Go to the directory where you plan to do your work.\n",
    "\n",
    "### Add a single volume\n",
    "If you're planning to analyze only a few volumes you can use the following command, replacing {{volume_id}} with your own:\n",
    "```\n",
    "htid2rsync {{volume_id}} | rsync -azv --files-from=- data.sharc.hathitrust.org::features/ local-folder/\n",
    "```\n",
    "\n",
    "### Add multiple volumes\n",
    "If you have a file of volume ids in a .txt file, with one ID per line, use --from-file filename, or just -f filename, and point to a text file with one volume ID on each line.\n",
    "```\n",
    "htid2rsync --f volumeids.txt | rsync -azv --files-from=- data.sharc.hathitrust.org::features/ local-folder/\n",
    "```\n",
    "\n",
    "### Sample datasets\n",
    "\n",
    "#### Complete Novels of Jane Austen (1 volume)\n",
    "```\n",
    "htid2rsync mdp.39015004788835 | rsync -azv --files-from=- data.sharc.hathitrust.org::features/ local-folder/\n",
    "```\n",
    "#### Nigerian Authors (30 volumes)\n",
    "authors-nigerian.txt includes volume IDs for 30 texts with the Library of Congress subject heading *Authors, Nigerian*. \n",
    "```\n",
    "htid2rsync --f authors-nigerian.txt | rsync -azv --files-from=- data.sharc.hathitrust.org::features/ local-folder/\n",
    "```\n",
    "\n",
    "#### San Francisco (Calif.) - History (111 volumes)\n",
    "sf-history.txt includes the volume ID for 111 texts with the Library of Congress subject heading *San Francisco (Calif.) - History*. \n",
    "```\n",
    "htid2rsync --f sf-history.txt | rsync -azv --files-from=- data.sharc.hathitrust.org::features/ local-folder/\n",
    "```\n",
    "\n",
    "#### Congressional Record (1200 volumes)\n",
    "congressional_record_ids.txt includes the volume ID for every *Congressional Record* volume that HathiTrust could share with us.\n",
    "```\n",
    "htid2rsync --f congressional_record_ids.txt | rsync -azv --files-from=- data.sharc.hathitrust.org::features/ local-folder/\n",
    "```\n",
    "\n",
    "#### Full 4TB library\n",
    "It's also possible to work with the entire library (4TB, so beware):\n",
    "```\n",
    "rsync -rv data.analytics.hathitrust.org::features/ .\n",
    "```\n",
    "\n",
    "Or to use existing lists of public-domain [fiction](http://data.analytics.hathitrust.org/genre/fiction_paths.txt), [drama](http://data.analytics.hathitrust.org/genre/drama_paths.txt), and [poetry](http://data.analytics.hathitrust.org/genre/poetry_paths.txt) (Underwood 2014).\n",
    "\n",
    "In the example, below, we have five volume IDs on San Francisco history from HathiTrust, which are listed in the file *vol_ids_5.txt.* You can modify the command to include your own list of volume ids or a single volume id of your choosing. (If you choose your own volume/s here, you will also need to modify the filepaths in the next step to point to those files).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sandbox] Welcome to the HathiTrust Research Center rsync server.\n",
      "\n",
      "receiving file list ... done\n",
      "coo/\n",
      "coo/pairtree_root/\n",
      "coo/pairtree_root/31/\n",
      "coo/pairtree_root/31/92/\n",
      "coo/pairtree_root/31/92/40/\n",
      "coo/pairtree_root/31/92/40/05/\n",
      "coo/pairtree_root/31/92/40/05/84/\n",
      "coo/pairtree_root/31/92/40/05/84/43/\n",
      "coo/pairtree_root/31/92/40/05/84/43/23/\n",
      "coo/pairtree_root/31/92/40/05/84/43/23/31924005844323/\n",
      "coo/pairtree_root/31/92/40/05/84/43/23/31924005844323/coo.31924005844323.json.bz2\n",
      "mdp/\n",
      "mdp/pairtree_root/\n",
      "mdp/pairtree_root/39/\n",
      "mdp/pairtree_root/39/01/\n",
      "mdp/pairtree_root/39/01/50/\n",
      "mdp/pairtree_root/39/01/50/18/\n",
      "mdp/pairtree_root/39/01/50/18/64/\n",
      "mdp/pairtree_root/39/01/50/18/64/26/\n",
      "mdp/pairtree_root/39/01/50/18/64/26/71/\n",
      "mdp/pairtree_root/39/01/50/18/64/26/71/39015018642671/\n",
      "mdp/pairtree_root/39/01/50/18/64/26/71/39015018642671/mdp.39015018642671.json.bz2\n",
      "uc1/\n",
      "uc1/pairtree_root/\n",
      "uc1/pairtree_root/32/\n",
      "uc1/pairtree_root/32/10/\n",
      "uc1/pairtree_root/32/10/60/\n",
      "uc1/pairtree_root/32/10/60/00/\n",
      "uc1/pairtree_root/32/10/60/00/66/\n",
      "uc1/pairtree_root/32/10/60/00/66/61/\n",
      "uc1/pairtree_root/32/10/60/00/66/61/20/\n",
      "uc1/pairtree_root/32/10/60/00/66/61/20/32106000666120/\n",
      "uc1/pairtree_root/32/10/60/00/66/61/20/32106000666120/uc1.32106000666120.json.bz2\n",
      "uc1/pairtree_root/32/10/60/14/\n",
      "uc1/pairtree_root/32/10/60/14/02/\n",
      "uc1/pairtree_root/32/10/60/14/02/99/\n",
      "uc1/pairtree_root/32/10/60/14/02/99/27/\n",
      "uc1/pairtree_root/32/10/60/14/02/99/27/32106014029927/\n",
      "uc1/pairtree_root/32/10/60/14/02/99/27/32106014029927/uc1.32106014029927.json.bz2\n",
      "uc1/pairtree_root/b3/\n",
      "uc1/pairtree_root/b3/62/\n",
      "uc1/pairtree_root/b3/62/49/\n",
      "uc1/pairtree_root/b3/62/49/67/\n",
      "uc1/pairtree_root/b3/62/49/67/b3624967/\n",
      "uc1/pairtree_root/b3/62/49/67/b3624967/uc1.b3624967.json.bz2\n",
      "\n",
      "sent 637 bytes  received 984,479 bytes  218,914.67 bytes/sec\n",
      "total size is 982,772  speedup is 1.00\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "htid2rsync --f vol_ids_5.txt | rsync -azv --files-from=- data.sharc.hathitrust.org::features/ local-folder/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Extracted Features\n",
    "All of the examples of code below are taken directly, or adapted, from the [Programming Historian tutorial](http://programminghistorian.org/lessons/text-mining-with-extracted-features) or the [FeatureReader's Readme.md file](https://github.com/htrc/htrc-feature-reader).\n",
    "\n",
    "You'll notice, from the output above, that the content for each volume is stored in a compressed JSON file, in a rather lengthy file directory. We can import and initialize FeatureReader with file paths pointing to those JSON files (using the full paths from the output above). If you chose to work with your own volumes in the previous step you can edit the cell below to add the paths from the output you see above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coo.31924005844323 A history of the city of San Francisco and incidentally of the State of California By John S. Hittell ...\n",
      "mdp.39015018642671 A cast of hawks : a rowdy tale of greed, violence, scandal, and corruption in the early days of San Francisco / by Milton S. Gould.\n",
      "uc1.32106000666120 Early days of San Francisco, California.\n",
      "uc1.32106014029927 Fire & gold : the San Francisco story / by Charles A. Fracchia\n",
      "uc1.b3624967 Committee of Vigilance: revolution in San Francisco, 1851;an account of the hundred days when certain citizens undertook the suppression of the criminal activities of the Sydney ducks.\n"
     ]
    }
   ],
   "source": [
    "from htrc_features import FeatureReader\n",
    "import os\n",
    "paths = [os.path.join('local-folder/', 'coo/pairtree_root/31/92/40/05/84/43/23/31924005844323/coo.31924005844323.json.bz2'), \n",
    "         os.path.join('local-folder/', 'mdp/pairtree_root/39/01/50/18/64/26/71/39015018642671/mdp.39015018642671.json.bz2'), \n",
    "         os.path.join('local-folder/', 'uc1/pairtree_root/32/10/60/00/66/61/20/32106000666120/uc1.32106000666120.json.bz2'), \n",
    "         os.path.join('local-folder/', 'uc1/pairtree_root/32/10/60/14/02/99/27/32106014029927/uc1.32106014029927.json.bz2'), \n",
    "         os.path.join('local-folder/', 'uc1/pairtree_root/b3/62/49/67/b3624967/uc1.b3624967.json.bz2')]\n",
    "fr = FeatureReader(paths)\n",
    "for vol in fr.volumes():\n",
    "    print(vol.id, vol.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to pull out some more metadata about these titles, using the [Volume object](http://htrc.github.io/htrc-feature-reader/htrc_features/feature_reader.m.html#htrc_features.feature_reader.Volume) in FeatureReader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL: http://hdl.handle.net/2027/coo.31924005844323 Year: 1878 Page count: 526 \n",
      "A.L. Bancroft, 1878.\n",
      "cau\n",
      "URL: http://hdl.handle.net/2027/mdp.39015018642671 Year: 1985 Page count: 382 \n",
      "Copley Books, c1985.\n",
      "cau\n",
      "URL: http://hdl.handle.net/2027/uc1.32106000666120 Year: 1949 Page count: 180 \n",
      "Biobooks, 1949.\n",
      "cau\n",
      "URL: http://hdl.handle.net/2027/uc1.32106014029927 Year: 1994 Page count: 408 \n",
      "Heritage Publishing Co. ; Lammert Publications, c1994\n",
      "cau\n",
      "URL: http://hdl.handle.net/2027/uc1.b3624967 Year: 1964 Page count: 352 \n",
      "Houghton Mifflin, 1964.\n",
      "mau\n"
     ]
    }
   ],
   "source": [
    "#show the HT URL, year, and page count for each volume\n",
    "for vol in fr.volumes():\n",
    "    print(\"URL: %s Year: %s Page count: %s \" % (vol.handle_url, vol.year, vol.page_count))\n",
    "    print(vol.imprint)\n",
    "    print(vol.pub_place)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mau'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vol.pub_place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#where were the volumes scanned\n",
    "for vol in fr.volumes():\n",
    "    print(\"Source institution: %s \" % (vol.source_institution))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#let's focus on the first volume\n",
    "vol = fr.first()\n",
    "print(vol.title)\n",
    "# and pull the tokens on each page\n",
    "tokens = vol.tokens_per_page()\n",
    "\n",
    "# Show just the first few rows, so we can look at what it looks like\n",
    "tokens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vol.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#we can easily plot the number of tokens across every page of the book\n",
    "%matplotlib inline\n",
    "tokens.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at some specific pages, using the [Page object in FeatureReader](http://htrc.github.io/htrc-feature-reader/htrc_features/feature_reader.m.html#htrc_features.feature_reader.Page)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "for page in vol:\n",
    "    # Same as `for page in vol.pages()`\n",
    "    i += 1\n",
    "    if i >= 200:\n",
    "        break\n",
    "print(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"The body has %s lines, %s empty lines, and %s sentences\" % (page.line_count(),\n",
    "                                                                   page.empty_line_count(),\n",
    "                                                                   page.sentence_count()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#look at the tokens on the page\n",
    "print(page.tokenlist()[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What else can we do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
